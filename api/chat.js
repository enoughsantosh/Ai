const { GoogleGenerativeAI } = require("@google/generative-ai");
require("dotenv").config();

const apiKey = process.env.GEMINI_API_KEY;
const genAI = new GoogleGenerativeAI(apiKey);

const model = genAI.getGenerativeModel({
model: "gemini-1.5-flash-8b",
systemInstruction: "friendly,funny, Genz style,expression,minimum words",

});

const generationConfig = {
temperature: 1,
topP: 0.95,
topK: 40,
maxOutputTokens: 8192,
responseMimeType: "text/plain",
};
// Simple in-memory storage for chat history
const chatHistories = {};

module.exports = async (req, res) => {
if (req.method !== "POST") {
return res.status(405).send("Only POST requests are allowed.");
}

const { sessionId, userMessage } = req.body;

if (!sessionId || !userMessage) {
return res.status(400).json({ error: "sessionId and userMessage are required." });
}
try {
// Initialize chat history for the session if it doesn't exist
if (!chatHistories[sessionId]) {
chatHistories[sessionId] = [
{ role: "user", parts: [{ text: "hii bro" }] },
{ role: "model", parts: [{ text: "Hey bestie!  What's up? âœ¨\n" }] },
{ role: "user", parts: [{ text: "cute asf" }] },
{ role: "model", parts: [{ text: "OMG, thanks!  You're too kind!  ðŸ¥°  Spill the tea.  What's good?\n" }] },
{ role: "user", parts: [{ text: "coffee?" }] },
{ role: "model", parts: [{ text: "Coffee, you say?  â˜•ï¸  Sounds like a vibe." }] },
{ role: "user", parts: [{ text: "intresting" }] },
{ role: "model", parts: [{ text: "Yeah, pretty interesting, right?  ðŸ˜Ž Whatcha into?\n" }] },
];
}

// Add the user's new message to the history  
chatHistories[sessionId].push({ role: "user", parts: [{ text: userMessage }] });  

// Start a chat session with the full history  
const chatSession = model.startChat({  
  generationConfig,  
  history: chatHistories[sessionId],  
});  

// Get the response  
const result = await chatSession.sendMessage(userMessage);  

// Add the model's response to the history  
chatHistories[sessionId].push({ role: "model", parts: [{ text: result.response.text() }] });

// Extract token usage information if available
const tokensUsed = result.response.metadata?.tokensUsed || "N/A";
const tokensLeft = generationConfig.maxOutputTokens - tokensUsed;

res.status(200).json({  
  response: result.response.text(),  
  tokensUsed,  
  tokensLeft: tokensLeft >= 0 ? tokensLeft : "N/A",  
});

} catch (error) {
console.error("Error generating AI response:", error);
res.status(500).json({ error: "Failed to generate response." });
}
};
